{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from nemo_curator.datasets.doc_dataset import DocumentDataset\n",
    "from nemo_curator.utils.config_utils import build_filter_pipeline\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import pandas as pd\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "from nemo_curator import Modify, ScoreFilter, Sequential\n",
    "from nemo_curator.modifiers import UnicodeReformatter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"BramVanroy/CommonCrawl-CreativeCommons\", \"CC-MAIN-2025-05-eng\") \n",
    "\n",
    "# Set the paths for the curation settings\n",
    "basic_config = \"../config/basic_curation.yaml\"\n",
    "c4_config = \"../config/c4_curation.yaml\"\n",
    "full_config = \"../config/full_curation.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DatasetDict to pandas DataFrame\n",
    "df = pd.DataFrame(ds['train'])\n",
    "# Rename language field to en \n",
    "df['language'] = \"en\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1.000.000 random lines from the dataset and split into 10 subsets\n",
    "parts = []\n",
    "random_sample = df.sample(1000000)\n",
    "for i in range(10):\n",
    "    df_shuffled = random_sample.sample(frac=1).reset_index(drop=True)\n",
    "    samples = np.array_split(df_shuffled, 10)\n",
    "    for j in range(10):\n",
    "        parts.append(samples[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the processed dataset\n",
    "random_sample.to_parquet(f\"cc-main-2025-05-eng-part.parquet\", engine='pyarrow', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load the saved dataset and split into 10 subsets\n",
    "df = pd.read_parquet(f\"cc-main-2025-05-eng-part.parquet\", engine='pyarrow')\n",
    "parts = []\n",
    "df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "samples = np.array_split(df_shuffled, 10)\n",
    "for j in range(10):\n",
    "    parts.append(samples[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline to detect and fix unicode problems\n",
    "initial_curation = Modify(UnicodeReformatter())\n",
    "\n",
    "#For each part, create a DocumentDataset\n",
    "ds_parts = []\n",
    "times = []\n",
    "\n",
    "for i, part in enumerate(parts):\n",
    "    print(\"Processing part \", i)\n",
    "    ds_part = DocumentDataset.from_pandas(part)\n",
    "    t0 = time.time()\n",
    "    cleaned_dataset = initial_curation(ds_part)\n",
    "    cleaned_output = cleaned_dataset.df.compute()\n",
    "    part = DocumentDataset.from_pandas(cleaned_output)\n",
    "    times.append(f\"Unicode reformatting took {time.time() - t0} s\")\n",
    "    ds_parts.append(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basics = []\n",
    "c4 = []\n",
    "full = []\n",
    "\n",
    "for i, part in enumerate(ds_parts):\n",
    "    print(f\"Processing chunk {i+1} of {len(ds_parts)}\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    # construct pipeline from config\n",
    "    basic_pipeline = build_filter_pipeline(basic_config)\n",
    "\n",
    "    print(\"Running basic pipeline...\")\n",
    "    # filter data and write to disk\n",
    "    basic_dataset = basic_pipeline(part)\n",
    "    basic = basic_dataset.df.compute()\n",
    "    basics.append(len(basic))\n",
    "    t0f = time.time()\n",
    "\n",
    "    times.append(t0f - t0)\n",
    "\n",
    "    print(\"Running C4 pipeline...\")\n",
    "    t1 = time.time()\n",
    "    c4_pipeline = build_filter_pipeline(c4_config)\n",
    "    c4_dataset = c4_pipeline(part)\n",
    "    c4_data = c4_dataset.df.compute()\n",
    "    c4.append(len(c4_data))\n",
    "    t1f = time.time()\n",
    "    times.append(t1f - t1)\n",
    "\n",
    "    print(\"Running full pipeline...\")\n",
    "    t2 = time.time()\n",
    "    full_pipeline = build_filter_pipeline(full_config)\n",
    "    full_dataset = full_pipeline(part)\n",
    "    full_data = full_dataset.df.compute()\n",
    "    full.append(len(full_data))\n",
    "    t2f = time.time()\n",
    "    times.append(t2f - t2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the current experiment results\n",
    "results_df = pd.DataFrame({\n",
    "    'Basic': basics,\n",
    "    'C4': c4, \n",
    "    'Full': full\n",
    "})\n",
    "\n",
    "# Add percentages relative to original chunk sizes\n",
    "original_sizes = [len(part) for part in parts]\n",
    "results_df['Original_Size'] = original_sizes\n",
    "results_df['Basic_Percentage'] = (results_df['Basic'] / results_df['Original_Size'] * 100).round(2)\n",
    "results_df['C4_Percentage'] = (results_df['C4'] / results_df['Original_Size'] * 100).round(2)\n",
    "results_df['Full_Percentage'] = (results_df['Full'] / results_df['Original_Size'] * 100).round(2)\n",
    "results_df['Set'] = [i+1 for i in range(len(basics))]\n",
    "\n",
    "# Add totals row for this experiment\n",
    "totals = pd.DataFrame({\n",
    "    'Set': ['Total'],\n",
    "    'Original_Size': [sum(original_sizes)],\n",
    "    'Basic': [sum(basics)],\n",
    "    'Basic_Percentage': [(sum(basics) / sum(original_sizes) * 100)],\n",
    "    'C4': [sum(c4)],\n",
    "    'C4_Percentage': [(sum(c4) / sum(original_sizes) * 100)],\n",
    "    'Full': [sum(full)],\n",
    "    'Full_Percentage': [(sum(full) / sum(original_sizes) * 100)],\n",
    "})\n",
    "\n",
    "results_df = pd.concat([results_df, totals], ignore_index=True)\n",
    "\n",
    "# Rearrange columns for better readability\n",
    "results_df = results_df[['Set', 'Original_Size', 'Basic', 'Basic_Percentage', \n",
    "                          'C4', 'C4_Percentage', 'Full', 'Full_Percentage']]\n",
    "\n",
    "# Check if results file exists\n",
    "results_path = './results.csv'\n",
    "if os.path.exists(results_path):\n",
    "    # Load existing results and append new ones\n",
    "    existing_results = pd.read_csv(results_path)\n",
    "    combined_results = pd.concat([existing_results, results_df], ignore_index=True)\n",
    "    combined_results.to_csv(results_path, index=False)\n",
    "    print(f\"Results appended to {os.path.abspath(results_path)}\")\n",
    "else:\n",
    "    # Create new results file\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"New results file created at {os.path.abspath(results_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with the results\n",
    "import random\n",
    "random_samples = basic.sample(10)\n",
    "print(\"Random samples from the basic dataset:\")\n",
    "print(random_samples['text'])\n",
    "\n",
    "# Selects 10 samples removed from the basic dataset\n",
    "not_basic_set = df_shuffled[~df_shuffled['text'].isin(basic['text'])].sample(10)\n",
    "print(\"Random samples from the original dataset that are not in the basic dataset:\")\n",
    "print(not_basic_set['text'])\n",
    "\n",
    "# Selects 10 samples removed from the c4 dataset\n",
    "not_c4_set = df_shuffled[~df_shuffled['text'].isin(c4_data['text'])].sample(10)\n",
    "\n",
    "# Selects 10 samples removed from the c4 dataset that were not removed in the basic dataset\n",
    "not_c4_samples = not_c4_set[~not_c4_set['text'].isin(not_basic_set['text'])].sample(10)\n",
    "print(\"Random samples from the original dataset that are not in the c4 dataset:\")\n",
    "print(not_c4_samples['text'])\n",
    "\n",
    "# Selects 10 samples removed from the full dataset\n",
    "not_full_set = df_shuffled[~df_shuffled['text'].isin(full_data['text'])].sample(10)\n",
    "# Selects 10 samples removed from the full dataset that were not removed in the c4 dataset\n",
    "not_full_samples = not_full_set[~not_full_set['text'].isin(not_c4_set['text'])].sample(10)\n",
    "print(\"Random samples from the original dataset that are not in the c4 dataset:\")\n",
    "print(not_full_samples['text'])\n",
    "\n",
    "#save not_basic_set, not_c4_samples and not_full_samples in a single csv file\n",
    "not_basic_set.to_csv(\"not_basic_set.csv\", index=False)\n",
    "not_c4_samples.to_csv(\"not_c4_samples.csv\", index=False)\n",
    "not_full_samples.to_csv(\"not_full_samples.csv\", index=False)\n",
    "#save all samples in a single csv file\n",
    "all_samples = pd.concat([random_samples, not_basic_set, not_c4_samples, not_full_samples], ignore_index=True)\n",
    "all_samples.to_csv(\"all_samples.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
